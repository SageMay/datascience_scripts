{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6e97c97-3230-4f63-948b-68316f132997",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "This notebook is for help with comparing distributions of data, focusing on Kolmogorove Smirnov (KS) test. The KS test is non-parameteric, which makes it particularly useful for data where the underlying distribution is not known. \n",
    "\n",
    "### References\n",
    "[1] Zeimbekakis, A., Schifano, E. D., & Yan, J. (2024). On Misuses of the Kolmogorov–Smirnov Test for One-Sample Goodness-of-Fit. The American Statistician, 78(4), 481-487.\n",
    "https://www.tandfonline.com/doi/full/10.1080/00031305.2024.2356095 \\\n",
    "[2] Büning, H. (2002). Robustness and power of modified Lepage, Kolmogorov-Smirnov and Crame´ r-von Mises two-sample tests. Journal of Applied Statistics, 29(6), 907-924.\n",
    "https://www.tandfonline.com/doi/abs/10.1080/02664760220136212\n",
    "\n",
    "### Tutorials\n",
    "1. How the KS test looks for differences between two distriubtions: \\\n",
    "https://towardsdatascience.com/understanding-kolmogorov-smirnov-ks-tests-for-data-drift-on-profiled-data-5c8317796f78/\n",
    "2. Dealing with auto-correlated data (MD data often is): \\\n",
    "   https://engineering.atspotify.com/2023/09/how-to-accurately-test-significance-with-difference-in-difference-models\n",
    "3. Potential pitfalls of the K-S test! \\\n",
    "   https://asaip.psu.edu/articles/beware-the-kolmogorov-smirnov-test/#:~:text=We%20recommend%20that%20the%20distribution%20of%20the,KS%20test%20in%20two%20or%20more%20dimensions.\n",
    "5. Why Anderson Darling may be better, and a concise explanation of K-S's strengths and limitations: \\\n",
    "https://asaip.psu.edu/articles/beware-the-kolmogorov-smirnov-test/#:~:text=We%20recommend%20that%20the%20distribution%20of%20the,KS%20test%20in%20two%20or%20more%20dimensions.\n",
    "6. Explaining reference [1]: \\\n",
    "https://www.reddit.com/r/statistics/comments/7j273q/still_dont_understand_why_the_pvalue_distribution/\n",
    "7. Explaining uniform distribution of p-values: \\\n",
    "https://www.reddit.com/r/statistics/comments/7j273q/still_dont_understand_why_the_pvalue_distribution/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0520dfe0-fd0b-4a2a-96d2-e05fc33a3fa3",
   "metadata": {},
   "source": [
    "## Common pitfalls for the Kolmogorov-Smirnov test\n",
    "* Data must be continuous [1] (consider if Chi-square Test if data is categorical)\n",
    "    - Rounding continuous data can invalidate the test\n",
    "    - \"Ties\", where two data points have the same value indicate that the underlying data generator does not create continuous data\n",
    "* When comparing your distribution to another distribution (e.g. to see if your data is normal), your independent distribution should not be created using the mean and standard deviation of your dataset, but should be independent.\n",
    "    - See tutorial explaining reference 1, or AI summary in cell for \"Parametric Bootstrap for Kolmogorove-Smirnov Test in Python\"\n",
    "* Data should be independent (this is a major problem for MD data, which is usually from a time series and autocorrelated. autocorrelation means that the data at time x-i has some predicitive value for the data at time x, which for a protein moving in time is true -- as i goes to zero, the position at time x-i approaches the position at time x. Since the test assumes independence, autocorrelation contributes to an over-estimation of significance, along with the overpowering from excessive sampling) For more on methods to deal with this, see tutorial 2.\n",
    "\n",
    "## Special considerations\n",
    "* Consider using Anderson-Darling if the important differences between the distributions are in the tails. It may even be more advisable to always use A-D test. (https://asaip.psu.edu/articles/beware-the-kolmogorov-smirnov-test/#:~:text=We%20recommend%20that%20the%20distribution%20of%20the,KS%20test%20in%20two%20or%20more%20dimensions.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1c020b-c2e2-46f2-8381-a4d5bc485cd8",
   "metadata": {},
   "source": [
    "# So what do we do. \n",
    "\n",
    "According to the blog post in tutorial [3], bootstrapping IS advisable. Nonetheless, there is mixed consensus on online forums, largely from the standpoint that it is entirely unclear what the sample size should be. There is consensus that confidence intervals are more important than using a p-value for hypothesis testing alone. At all times, it is important to keep in mind that statistics are more a way to characterize the data rather than to prove a claim. \n",
    "\n",
    "It is ambiguous if bootstrapping might be helpful in resolving the auto-correlation issue. My thought right now is maybe the raw data needs to be analyzed for an autocorrelateion period, and that the sample size should be based on sampling roughly one sample per period. Arguably, this would result in lots of instances where a sinlge period is sampled multiple times, so there would be auto-correlations still, but at the same time it's the only reason I can conjure to justify a given sample size.\n",
    "\n",
    "It may also be possible to do a parameter sweep over sample size. But there's the question of what is the \"right\" answer, even if we did this -- we'd just be looking at a series of plots and picking whatever we like best. \n",
    "\n",
    "There seems to be no reason to not use either A-D or Cramer-von Mises in place of K-S. The reason that C-M may be preferable is it normalizes across the whole distribution rather than relying on the maximum alone. Any of these can be reweighted in order to emphasize the underlying upper/lower distributions. \n",
    "\n",
    "It may be worthwhile to bootstrap on Z and look at distribution of P values. It should be linear, flat. And then to bootstrap for X and Y -- it should be non-linear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ba7790-ae8f-41a9-8ca4-e43b51869db8",
   "metadata": {},
   "source": [
    "## Parametric Bootstrap for Kolmogorove-Smirnov Test in Python\n",
    "---\n",
    "Parametric Bootstrap for Kolmogorov-Smirnov (KS) Test in Python\n",
    "The Kolmogorov-Smirnov (KS) test is a non-parametric test used to assess if a sample comes from a specific distribution (one-sample KS test) or if two samples come from the same distribution (two-sample KS test). While it's non-parametric, you can use the parametric bootstrap to approximate the null distribution of the KS test statistic when working with a parametric family of distributions. \n",
    "Why use Parametric Bootstrap for KS Test?\n",
    "The standard KS test assumes that the parameters of the hypothesized distribution are known. However, in practice, these parameters are often estimated from the sample data. When estimated parameters are used in the KS test, the test statistic's null distribution can change, leading to a conservative test (meaning you're less likely to reject the null hypothesis than you should be). \n",
    "The parametric bootstrap helps address this by providing a more accurate approximation of the null distribution of the KS test statistic when parameters are estimated. \n",
    "Steps for Parametric Bootstrap of KS Test in Python\n",
    "\n",
    "    Fit the parametric distribution: Choose a parametric distribution that you believe the data follows and estimate its parameters from your data.\n",
    "    Generate bootstrap samples: Repeatedly (e.g., 1000 times) draw random samples of the same size as your original data from the fitted distribution (using the estimated parameters).\n",
    "    Calculate the KS test statistic for each bootstrap sample: For each bootstrap sample, perform a KS test against the fitted distribution (using the estimated parameters) and record the KS test statistic (D).\n",
    "    Approximate the null distribution: The collection of KS test statistics from the bootstrap samples forms an empirical distribution that approximates the null distribution of the KS test statistic when parameters are estimated.\n",
    "    Determine the p-value: Compare your original data's KS test statistic to the bootstrap distribution of KS statistics to get a more accurate p-value for your hypothesis test. \n",
    "\n",
    "Python Implementation\n",
    "You can implement this using libraries like NumPy and SciPy in Python. \n",
    "\n",
    "    Use NumPy to generate random data and calculate statistics.\n",
    "    SciPy's scipy.stats.kstest function can be used to perform the KS test.\n",
    "    You'll need to define a function that performs the parametric bootstrap steps, including fitting the distribution, generating bootstrap samples, and calculating the KS statistic for each sample. \n",
    "\n",
    "Example using scipy.stats.kstest:\n",
    "The scipy.stats.kstest function can be used for the KS test in Python. It takes the data, the hypothesized distribution's CDF (either a string name or a callable function), and optional parameters. SciPy documentation shows you how to use it. \n",
    "You can implement the parametric bootstrap by:\n",
    "\n",
    "    Estimating parameters: Use methods like Maximum Likelihood Estimation (MLE) or Method of Moments to estimate the parameters of the chosen distribution from your data.\n",
    "    Creating a custom CDF function: Define a Python function that calculates the cumulative distribution function (CDF) of your chosen parametric distribution using the estimated parameters.\n",
    "    Generating bootstrap samples and KS statistics: Inside a loop, generate random samples from the fitted distribution using the estimated parameters. For each sample, call scipy.stats.kstest using your custom CDF function and store the resulting KS statistic.\n",
    "    Analyzing bootstrap results: After the loop, analyze the distribution of the stored KS statistics to get a bootstrap-based p-value for your test. \n",
    "\n",
    "Important Notes:\n",
    "\n",
    "    Choosing the appropriate parametric distribution is crucial for the parametric bootstrap to provide accurate results.\n",
    "    The number of bootstrap samples (B) should be sufficiently large to accurately approximate the null distribution (e.g., B ≥ 1000).\n",
    "    If your underlying data doesn't closely follow the assumed parametric distribution, the parametric bootstrap results might not be reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b927333a-8326-43c2-bc60-0408010d3002",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
